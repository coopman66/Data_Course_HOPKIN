qual.1 = quality(fq.1)
# show read IDs
id(fq.1)
ids.1 = id(fq.1)
# Finding patterns (e.g., to remove primer sequences or whatever)
primer = "GTAGTCCACGCCGTAAA"
matches = vmatchPattern(pattern = primer, subject = reads.1, fixed=TRUE)
reads.1[matches]
# trimming low-quality reads from ends of ShortRead objects
quality(fq.1)
alphabet(quality(fq.1))
trimmed = trimEnds(fq.1, a = "H")
quality(trimmed)
sread(trimmed)
# get frequencies of nucleotides in fastq reads
nucl.freq = alphabetFrequency(sread(fq.1), as.prob = TRUE, collapse = TRUE)
# Look at most common reads in file
tables(fq.1)
# get quality scores as a matrix
qual = as(quality(fq.1), "matrix")
# plot mean quality scores by cycle
plot(colMeans(qual), type = "b")
# Convert fastq to fasta and write them to files in same directory
writeFasta(fq.1,"fq1.fasta")
writeFasta(fq.2,"fq2.fasta")
# Get fasta files (the ones you just wrote to disk)
fa.files = dir(getwd(), full.names = TRUE, pattern = ".fasta")
# import fasta files, formatted for sequence alignment
fa.1 = readDNAStringSet(fa.files[1])
fa.2 = readDNAStringSet(fa.files[2])
##### perform multiple sequence alignment with default settings #####
align1 = msa(fa.1)
align2 = msa(fa.2)
# if you want to print the full alignment and/or save as a text file
print(align1, show = "complete") # you could wrap this in sink() to save it
# if this is a phylogenetically informative region of DNA, you can build a tree from these alignments
# but first you need to decide what tree package you want and convert the alignment to that format
# we will use phangorn
library(phangorn)
# convert to seqinr format...
seq_align1 = msaConvert(align1, type="phangorn::phyDat")
seq_align2 = msaConvert(align2, type="phangorn::phyDat")
# explore the fasta alignment objects
summary(seq_align1)
# Build a distance-matrix (comparing "differences between each pair of sequences)
# several ways of doing this .. we will use "maximum likelihood"
dm1 = dist.ml(seq_align1)
dm2 = dist.ml(seq_align2)
#check it out
head(dm1,20)
# each number is the "distance" between the sequences indicated in "rows" and "columns"
View(as.matrix(dm1))
heatmap(as.matrix(dm1))
# We can use the values to compute a phylogenetic tree
# first, we will try a "Neighbor-Joining" tree method
NJ.1 = NJ(dm1)
NJ.1
NJ.2 = NJ(dm2)
# A quick way to plot a tree
plot(NJ.1)
plot(NJ.2)
# This distance matrix method is computationally cheap, but loses some information (i.e., the actual sequences)
# To keep the sequences under consideration...
# compute liklihood of a given tree
fit <- pml(NJ.1, seq_align1)
print(fit)
# find optimum...using Jukes-Cantor nucleotide model (way too many options to cover them all)
fitJC = optim.pml(fit, model = "JC", rearrangement = "stochastic")
?dist
# log-liklihood value
logLik(fitJC)
# Use that tree as a starting point, and randomly resample 100 times to compute bootstrap values
bs <- bootstrap.pml(fitJC, bs=100, optNni=TRUE, multicore=TRUE, control = pml.control(trace=0))
plotBS(midpoint(fitJC$tree), bs, p = 50, type="p")
# Best way to make a tree look pretty is with a specialized tree-editing program like "FigTree"
# Export this tree
write.tree(bs, file="seq_align1.tre")
library(dada2)
plotQualityProfile(fq.files[1])
# get filenames for files without path
filt.files = dir(path = getwd(), full.names = FALSE, pattern = ".fastq")
# make new directory
dir.create(file.path(getwd(),"filtered"))
# stringent quality filter to remove N's etc
for(i in filt.files){
fastqFilter(fn = i,fout = paste0(getwd(),"/filtered/",i,".filt"),
truncLen = 200, trimLeft = 20)
}
setwd(file.path(paste0(getwd(),"/filtered")))
filtered = dir(path = getwd(), pattern = ".filt")
dr = derepFastq(filtered)
dr$F3D0_S188_L001_R1_001.fastq.filt$uniques
err = learnErrors(dr)
plotErrors(err)
log10(0.1)
-4^10
log10(1)
log10(.001)
log10(.0001)
?plotErrors
plotErrors(err, err_out = TRUE, err_in = TRUE)
plotErrors(err, err_in = TRUE, nominalQ = TRUE)
clean = dada(dr,err)
SeqTable = makeSequenceTable(clean)
taxonomy = assignTaxonomy(SeqTable, refFasta = "../rdp_train_set_16.fa.gz")
fa.1
fa.1[1]
rm(list = ls())
library(vegan)
set.seed(55)
# Generate a random community matrix, similar to the type of data in an OTU Table
prob = c(0.99,rep(0.05,1000)) # sets probability of each number being randomly chosen
community_matrix=matrix(
sample(0:1000,300,replace=T, prob = prob),nrow=10,
dimnames=list(paste("Sample_",1:10,sep=""),paste("OTU_",1:30,sep="")))
# Add a treatment vector, assigning samples to one of two groups
treat=c(rep("Treatment_1",5),rep("Treatment_2",5))
# Take a quick look at your "OTU Table"
community_matrix  # should have 10 samples as rows, and 30 OTUs as columns
# Check to see how even your sampling effort for each community is
barchart(rowSums(community_matrix))
min_depth = min(rowSums(community_matrix)) # this gives us the minimum number of reads in a given sample
min_depth
rarecurve(community_matrix, step = 100, sample = min_depth) # vegan's built-in S.A.C. function
# We should normalize our data to account for variable sequencing depth.  One way is to rarefy...
set.seed(55) # set random seed so it's reproducible
rare_community_matrix = rrarefy(community_matrix, min_depth) # randomly subsamples data to given depth and makes new OTU table
rare_community_matrix
specnumber(community_matrix)
specnumber(rare_community_matrix)
# How many species were found in each treatment group?
specnumber(community_matrix, groups = treat)
specnumber(rare_community_matrix, groups = treat)
# Calculate Shannon diversity for each sample
diversity(community_matrix, index = "shannon")
diversity(rare_community_matrix, index = "shannon")
?diversity
beta_div = betadiver(community_matrix, method = "w")
beta_div
?betadiver
# Beta diversity (basic Whittaker index)
beta_div = betadiver(community_matrix, method = "w")
beta_disp = betadisper(beta_div, treat)
beta_disp
plot(beta_disp)
?betadisper
beta_div
beta_disp
plot(beta_disp)
example_NMDS = metaMDS(rare_community_matrix, # Our community-by-species matrix
k=2) # The number of reduced dimensions
example_NMDS
stressplot(example_NMDS) # Lots of scatter means poor preservation
# Plot the ordination
ordiplot(example_NMDS, type = "n") # Sets up the plotting area
orditorp(example_NMDS,display="sites",col="red",air=0.01) # Adds the samples in ordination space
ordiellipse(example_NMDS, groups = treat, label = TRUE) # Calculates the centroid and 95% C.I. of each treatment group
# What you are looking at is a plot of community similarity between your samples that has been reduced to 2D
# Samples that are closer together in the plot have more similar community compositions
# The ellipses overlap, meaning there appears to be little to distinguish communities based on Treatment.
# This is not unexpected, given that our data are fake, and randomly drawn from the same distribution.
################################################################################
#                   Loading data into R - Proper Format                        #
################################################################################
# It is possible to directly import the biom file provided by QIIME, but this depends on the "biomformat"
# and "Matrix" packages, and versions and dependencies can get confusing, so we will start with a tab-separated OTU table.
# To prepare your biom file you need to convert it to tab-separated format and clean up the first line
# On the command line, you can do the following:
# biom convert --to-tsv --table-type "OTU table" -i otu_table_mc2_w_tax.biom -o otu_table_mc2_w_tax.tsv
# tail -n +2 otu_table_mc2_w_tax.tsv | sed 's/^#OTU ID/OTU_ID/'| head -1 > otu_table.tsv
# those two lines will convert your biom to tsv and clean up the first 2 lines of the file so R can read it easily
# the other file you will need is the sample metadata.  Typically, you will make this in excel, and it has sample IDs as rows
# and information about each sample as columns.
# For this tutorial we're going to start with an example dataset
# To read a tsv file into R we can do the following:
otu_table = read.delim("~/Desktop/GIT_REPOSITORIES/Data_Course/code_examples/Vegan_Example/vegan_example_otu_table.tsv", sep = "\t", row.names = 1)
metadata = read.delim("~/Desktop/GIT_REPOSITORIES/Data_Course/code_examples/Vegan_Example/vegan_example_metadata.tsv", sep = "\t", row.names = 1)
# The OTU table from QIIME, by default, has samples/sites as columns and OTUs/Species as rows.
# This is fine, but we will need to transpose it in order to use vegan.
# First, look at the dimensions of the two tables:
dim(otu_table)
dim(metadata)
metadata
dim(metadata)
otu_table
dim(otu_table)
names(otu_table) # that last one is the taxonomy from QIIME
Taxonomy = data.frame(Taxonomy = otu_table$Tax_Name, row.names = rownames(otu_table))
# And now, get rid of them from the otu table for now so we can use vegan
otu_table = otu_table[,which(colnames(otu_table) != "Tax_Name")]
# Now, transpose the otu table so rows are samples, and OTUs are columns
otu_table = as.data.frame(t(otu_table))
# Now, let's check the dimensions again
dim(otu_table)
dim(metadata)
otu_table
dim(metadata)
identical(rownames(otu_table), rownames(metadata)) # if "TRUE", then they are exactly the same, and in the same order
barchart(rowSums(otu_table))
min_depth2 = min(rowSums(otu_table)) # should be 2000 for this data set
min_depth2
rarecurve(otu_table, sample = min_depth2, step = 100, cex = .5)
# Normalize the data by rarefying
set.seed(1)
rare_otu_table = rrarefy(otu_table, min_depth2)
# How many OTUs are found in each sample?
specnumber(otu_table)
specnumber(rare_otu_table)
# compare before and after rarefication
plot(specnumber(otu_table),specnumber(rare_otu_table)) # there was some loss of species richness during rarification
# Look at it by groups (Ecosystem)
specnumber(otu_table, groups = metadata$Ecosystem) # refers to the metadata table
specnumber(rare_otu_table, groups = metadata$Ecosystem) # refers to the metadata table
# Look at Shannon Diversity
diversity(otu_table, "shannon")
diversity(rare_otu_table, "shannon")
# compare before and after rarefication
plot(diversity(otu_table, "shannon"),
diversity(rare_otu_table, "shannon"))
# Find the number of samples in which each OTU is found
species_presence = apply(rare_otu_table > 0,2,sum)
barchart(species_presence)
rare_otu_table > 0
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
View(df)
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
View(df)
which(df$Diet == NA)
which(df$Diet == "NA")
complete.cases(df)
complete.cases(df[,1:12])
df = complete.cases(df[,1:12])
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
which(complete.cases(df[,1:12]) == TRUE)
complete = which(complete.cases(df[,1:12]) == TRUE)
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
complete = which(complete.cases(df[,1:12]) == TRUE)
dat = df[complete,]
96*2
94*2
levels(dat$Gender)
View(dat)
levels(dat$Ethnicity)
dim(dat)
196-188
dim(dat) # we have 196 samples left...maximum should be 188 so we need to lose 8 more somehow
dat$Diet != "Omnivore"
which(dat$Diet != "Omnivore" | dat$Diet != "Vegetarian")
which(dat$Diet != "Omnivore"|"Vegetarian")
levels(dat$Diet)
dat$Diet %in% c("Omnivore","Vegetarian")
which(dat$Diet %in% c("Omnivore","Vegetarian"))
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian"))
dat = dat[omniveg,]
dim(dat)
sample(dat,10)
sample(dat,10)
row.names(dat)
set.seed(123)
sample(row.names(dat),188)
sample(row.names(dat),188, replace = FALSE)
sample(row.names(dat),188, replace = FALSE)
set.seed(123)
sample(row.names(dat),188, replace = FALSE)
set.seed(123)
sample(row.names(dat),188, replace = FALSE)
set.seed(123)
sample(as.numeric(row.names(dat)),188, replace = FALSE)
set.seed(123)
randomrows = sample(as.numeric(row.names(dat)),188, replace = FALSE)
dat = dat[randomrows,]
View(dat)
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
#remove all rows that are missing any data in the first 12 columns
complete = which(complete.cases(df[,1:12]) == TRUE)
dat = df[complete,]
# look at dimensions of our data frame (rows, columns)
dim(dat) # we have 196 samples left...maximum should be 188 so we need to lose 8 more somehow
# remove any rows where diet isn't "Vegetarian" or "Omnivore"
levels(dat$Diet) # look at options in the data
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian")) # find all rows where diet is omnivore or vegetarian
# subset, removing non-veg and non-omnivore
dat = dat[omniveg,]
dim(dat) # down to 191 samples....still need to remove a few
# let's do it randomly
set.seed(123)
randomrows = sample(as.numeric(row.names(dat)),188, replace = FALSE)
# subset data frame to the randomly selected rows
dat = dat[randomrows,]
View(dat)
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian")) # find all rows where diet is omnivore or vegetarian
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
#remove all rows that are missing any data in the first 12 columns
complete = which(complete.cases(df[,1:12]) == TRUE)
dat = df[complete,]
# look at dimensions of our data frame (rows, columns)
dim(dat) # we have 196 samples left...maximum should be 188 so we need to lose 8 more somehow
View(dat)
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian")) # find all rows where diet is omnivore or vegetarian
dat = dat[omniveg,]
dim(dat) # down to 191 samples....still need to remove a few
View(dat)
set.seed(123)
randomrows = sample(as.numeric(row.names(dat)),188, replace = FALSE)
randomrows
dat[73,]
dat$Diet
randomrows
dat[randomrows,]
dat[73,]
unique(dat$Gender)
randomrows = sample(as.numeric(row.names(dat)),188, replace = FALSE)
randomrows
set.seed(123)
randomrows = sample(as.numeric(row.names(dat)),188, replace = FALSE)
randomrows
randomdat = dat[randomrows,]
View(randomdat)
randomrows = sample((row.names(dat)),188, replace = FALSE)
unique(dat$Gender)
# subset data frame to the randomly selected rows
randomdat = dat[randomrows,]
set.seed(123)
randomrows = sample((row.names(dat)),188, replace = FALSE)
dat = dat[randomrows,]
View(dat)
order(dat$SampleID)
dat[order(dat$SampleID),]
dat = dat[order(dat$SampleID),]
plot(dat$Ethnicity, dat$WeeklyAlcohol)
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv", stringsAsFactors = FALSE)
#remove all rows that are missing any data in the first 12 columns
complete = which(complete.cases(df[,1:12]) == TRUE)
dat = df[complete,]
# look at dimensions of our data frame (rows, columns)
dim(dat) # we have 196 samples left...maximum should be 188 so we need to lose 8 more somehow
# remove any rows where diet isn't "Vegetarian" or "Omnivore"
levels(dat$Diet) # look at options in the data
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian")) # find all rows where diet is omnivore or vegetarian
# subset, removing non-veg and non-omnivore
dat = dat[omniveg,]
dim(dat) # down to 191 samples....still need to remove a few
# let's do it randomly
set.seed(123)
randomrows = sample((row.names(dat)),188, replace = FALSE)
# subset data frame to the randomly selected rows
dat = dat[randomrows,]
# rearrange to correct order for ease of sample processing
dat = dat[order(dat$SampleID),]
plot(dat$Ethnicity, dat$WeeklyAlcohol)
View(dat)
names(df)
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
names(df)
#remove all rows that are missing any data in the first 12 columns
complete = which(complete.cases(df[,1:12]) == TRUE)
dat = df[complete,]
# look at dimensions of our data frame (rows, columns)
dim(dat) # we have 196 samples left...maximum should be 188 so we need to lose 8 more somehow
# remove any rows where diet isn't "Vegetarian" or "Omnivore"
levels(dat$Diet) # look at options in the data
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian")) # find all rows where diet is omnivore or vegetarian
# subset, removing non-veg and non-omnivore
dat = dat[omniveg,]
dim(dat) # down to 191 samples....still need to remove a few
# let's do it randomly
set.seed(123)
randomrows = sample((row.names(dat)),188, replace = FALSE)
# subset data frame to the randomly selected rows
dat = dat[randomrows,]
# rearrange to correct order for ease of sample processing
dat = dat[order(dat$SampleID),]
plot(dat$Ethnicity, dat$WeeklyAlcohol)
dat$Diet = as.factor(as.character(dat$Diet))
levels(dat$Diet)
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
names(df)
#remove all rows that are missing any data in the first 12 columns
complete = which(complete.cases(df[,1:12]) == TRUE)
dat = df[complete,]
# look at dimensions of our data frame (rows, columns)
dim(dat) # we have 196 samples left...maximum should be 188 so we need to lose 8 more somehow
# remove any rows where diet isn't "Vegetarian" or "Omnivore"
levels(dat$Diet) # look at options in the data
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian")) # find all rows where diet is omnivore or vegetarian
# subset, removing non-veg and non-omnivore
dat = dat[omniveg,]
dim(dat) # down to 191 samples....still need to remove a few
# let's do it randomly
set.seed(123)
randomrows = sample((row.names(dat)),188, replace = FALSE)
# subset data frame to the randomly selected rows
dat = dat[randomrows,]
# rearrange to correct order for ease of sample processing
dat = dat[order(dat$SampleID),]
levels(dat$Diet)
dat$Diet = as.factor(as.character(dat$Diet))
dat$ActivityLevel = as.factor(as.character(dat$ActivityLevel))
dat$Gender = as.factor(as.character(dat$Gender))
dat$Ethnicity = as.factor(as.character(dat$Ethnicity))
dat$WeeklyChewTobacco = as.factor(as.character(dat$WeeklyChewTobacco))
df = read.csv("~/Downloads/Oral Fungi Survey - Sheet1.csv")
names(df)
#remove all rows that are missing any data in the first 12 columns
complete = which(complete.cases(df[,1:12]) == TRUE)
dat = df[complete,]
# look at dimensions of our data frame (rows, columns)
dim(dat) # we have 196 samples left...maximum should be 188 so we need to lose 8 more somehow
# remove any rows where diet isn't "Vegetarian" or "Omnivore"
levels(dat$Diet) # look at options in the data
omniveg = which(dat$Diet %in% c("Omnivore","Vegetarian")) # find all rows where diet is omnivore or vegetarian
# subset, removing non-veg and non-omnivore
dat = dat[omniveg,]
dim(dat) # down to 191 samples....still need to remove a few
# let's do it randomly
set.seed(123)
randomrows = sample((row.names(dat)),188, replace = FALSE)
# subset data frame to the randomly selected rows
dat = dat[randomrows,]
# rearrange to correct order for ease of sample processing
dat = dat[order(dat$SampleID),]
# reclassify columns as new factors where needed
dat$Diet = as.factor(as.character(dat$Diet))
dat$ActivityLevel = as.factor(as.character(dat$ActivityLevel))
dat$Gender = as.factor(as.character(dat$Gender))
dat$Ethnicity = as.factor(as.character(dat$Ethnicity))
plot(dat$Ethnicity, dat$WeeklyAlcohol)
plot(dat$Ethnicity, dat$WeeklyAlcohol, main = "Alcohol use by ethnicity")
plot(dat$Ethnicity, dat$WeeklyCigarettes)
plot(dat$Ethnicity, dat$)
plot(dat$Ethnicity, dat$WeeklyVape)
plot(dat$Ethnicity, dat$Diet)
plot(dat$Ethnicity, dat$ActivityLevel)
plot(dat$Ethnicity, dat$Age)
plot(dat$Diet, dat$Age)
plot(dat$Gender, dat$Age)
plot(dat$Gender, dat$Age)
plot(dat$Gender, dat$ActivityLevel)
plot(dat$Gender, dat$WeeklyAlcohol)
plot(dat$WeeklyAlcohol ~ dat$Ethnicity*dat$Diet)
plot(dat$WeeklyAlcohol ~ dat$Ethnicity*dat$Age)
plot(dat$WeeklyAlcohol ~ dat$Age)
plot(dat$WeeklyAlcohol ~ dat$Age, main = "Alcohol use by age")
library(ggplot2)
library(dplyr)
names(dat)
ggplot(dat, aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth()
ggplot(dat, aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE)
levels(dat$Ethnicity)
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4]))
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(dat, aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE)
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE)
# look at alcohol use for fun
plot(dat$Ethnicity, dat$WeeklyAlcohol, main = "Alcohol use by ethnicity")
plot(dat$WeeklyAlcohol ~ dat$Age, main = "Alcohol use by age")
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE)
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Diet,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE)
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=ActivityLevel,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE)
plot(dat$ActivityLevel, dat$Ethnicity)
plot(dat$ActivityLevel ~ dat$Age)
class(dat$Age)
plot(dat$ActivityLevel ~ as.numeric(dat$Age))
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE)
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_point() + geom_smooth(se = FALSE) +
theme_bw() + ggtitle("Weekly Alcohol Use by Age, Colored by Ethnicity")
ggsave("~/Desktop/oral_fungi_example_plot.png")
print(dat$SampleID)
print(as.character(dat$SampleID))
sink("~/Desktop/UVU/Research/Oral_Mycobiome/samples_to_use.txt")
print(as.character(dat$SampleID))
sink(NULL)
write.csv(dat$SampleID, file = "~/Desktop/UVU/Research/Oral_Mycobiome/samples_to_use.txt", row.names = FALSE)
write.csv(dat$SampleID, file = "~/Desktop/UVU/Research/Oral_Mycobiome/samples_to_use.txt", row.names = FALSE, col.names = FALSE)
write.csv(dat$SampleID, file = "~/Desktop/UVU/Research/Oral_Mycobiome/samples_to_use.txt", row.names = FALSE, col.names = "SampleID")
write.csv(dat$SampleID, file = "~/Desktop/UVU/Research/Oral_Mycobiome/samples_to_use.txt", row.names = FALSE,
col.names = "SampleID", quote = FALSE)
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_jitter() + geom_smooth(se = FALSE) +
theme_bw() + ggtitle("Weekly Alcohol Use by Age, Colored by Ethnicity")
?geom_jitter
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Age,y=WeeklyAlcohol,color=Ethnicity)) +
geom_jitter(height = 0) + geom_smooth(se = FALSE) +
theme_bw() + ggtitle("Weekly Alcohol Use by Age, Colored by Ethnicity")
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Ethnicity,y=WeeklyAlcohol,color=Ethnicity)) +
geom_jitter(height = 0) + geom_smooth(se = FALSE) +
theme_bw() + ggtitle("Weekly Alcohol Use by Age, Colored by Ethnicity")
ggplot(aes(x=Ethnicity,y=WeeklyAlcohol,color=Ethnicity)) +
geom_violin()
ggplot(dat, aes(x=Ethnicity,y=WeeklyAlcohol,color=Ethnicity)) +
geom_violin()
subset(dat, Ethnicity %in% c(levels(dat$Ethnicity)[1:4])) %>%
ggplot(aes(x=Ethnicity,y=WeeklyAlcohol,color=Ethnicity)) +
geom_violin()
